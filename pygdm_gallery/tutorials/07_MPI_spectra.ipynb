{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutorial: MPI-parallelized calculation of spectra\n",
    "=================\n",
    "In this short tutorial we demonstrate how to accelerate the calculation of spectra via MPI multi-processing parallelization.\n",
    "\n",
    "Load modules\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## --- for benchmark: limit openmp to 1 thread (for parallel scipy/numpy routines)\n",
    "##     Must be done before loading numpy to have an effect\n",
    "\n",
    "import os\n",
    "nthreads = 1\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"{}\".format(int(nthreads))\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"{}\".format(int(nthreads))\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"{}\".format(int(nthreads))\n",
    "\n",
    "## --- load pyGDM modules\n",
    "from pyGDM2 import structures\n",
    "from pyGDM2 import materials\n",
    "from pyGDM2 import fields\n",
    "\n",
    "from pyGDM2 import core\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "## --- Note: It is not necessary to load mpi4py within the simulation script, this \n",
    "## --- will be done automatically by pyGDM2 prior to the actual MPI-simulation. \n",
    "## --- We do it however at this point to do some output to stdout only from \n",
    "## --- the master process (rank == 0).\n",
    "from mpi4py import MPI\n",
    "rank = MPI.COMM_WORLD.rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Config of the simulation\n",
    "--------------------------------------\n",
    "\n",
    "We will demonstrate the MPI spectra calculation on the simple example of a small dielectric sphere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## ---------- Setup structure\n",
    "mesh = 'cube'\n",
    "step = 20.0\n",
    "radius = 3.5\n",
    "geometry = structures.sphere(step, R=radius, mesh=mesh)\n",
    "material = materials.dummy(2.0)\n",
    "n1, n2 = 1.0, 1.0\n",
    "struct = structures.struct(step, geometry, material, n1,n2, \n",
    "                           structures.get_normalization(mesh))\n",
    "\n",
    "## ---------- Setup incident field\n",
    "field_generator = fields.planewave\n",
    "wavelengths = np.linspace(400, 800, 20)\n",
    "kwargs = dict(theta = [0.0])\n",
    "efield = fields.efield(field_generator, wavelengths=wavelengths, kwargs=kwargs)\n",
    "\n",
    "## ---------- Simulation initialization\n",
    "sim = core.simulation(struct, efield)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the simulation with the MPI wrapper to *core.scatter*\n",
    "---------------------------------------------\n",
    "\n",
    "The only difference to a non MPI-parallelized run of the simulation is, that we use *core.scatter_mpi* instead of *core.scatter*. *scatter_mpi* will automatically distribute the calculation of the different wavelengths in the spectrum on the available processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## --- mpi: print in process with rank=0, to avoid flooding of stdout\n",
    "if rank == 0: \n",
    "    print(\"performing MPI parallel simulation... \")\n",
    "\n",
    "core.scatter_mpi(sim)\n",
    "\n",
    "if rank == 0: \n",
    "    print(\"simulation done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT NOTE:** In order to be run by MPI, the script needs to be executed by the program **mpirun**:\n",
    "\n",
    "    $ mpirun -n 4 python pygdm_script_using_mpi.py\n",
    "\n",
    "where in this example, the argument \"-n 4\" tells MPI to run 4 parallel processes.\n",
    "\n",
    "**Note**: in case the number of wavelengths in the spectra is not divisable by the number of MPI processes, some MPI-processes will be idle for some time during execution. In this case *scatter_mpi* will return a warning:\n",
    "\n",
    "    UserWarning: Efficiency warning: Number of wavelengths (20) not divisable by Nr of processes (3)!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Timing of the above example\n",
    "--------------------\n",
    " \n",
    " - MPI-run:         1.25 s\n",
    " - sequential-run:  4.05 s\n",
    " - speed-up:        x3.2\n",
    " \n",
    "This should be more close to x4 in case of a larger simulation, where the MPI overhead becomes small compared to the simulation runtime."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
